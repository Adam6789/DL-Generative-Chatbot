{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### title:\n",
    "\n",
    "### project overview:\n",
    "\n",
    "### dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_chatbot import questions_answers, load_df, toTensor, show_lengths, tokenize_questions,tokenize_answers\n",
    "from src.data_chatbot import pretrained_w2v, prepare_text\n",
    "from src.models_chatbot import Seq2Seq\n",
    "from src.vocab_chatbot import Vocab\n",
    "\n",
    "\n",
    "from src.train_chatbot import pretrain, train\n",
    "from src.apply_chatbot import apply_chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest -vv src/tests_chatbot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_df()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train_raw, questions_valid_raw, answers_train_raw, answers_valid_raw = questions_answers(source_name=source_name)\n",
    "show_lengths(questions_train_raw, questions_valid_raw, answers_train_raw, answers_valid_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a desicion has to be made between the next and the after next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = [pair for pair in zip(questions_train_raw, answers_train_raw) if len(pair[1])>3]\n",
    "# questions_train_filt, answers_train_filt = map(list, zip(*temp))\n",
    "# temp = [pair for pair in zip(questions_valid_raw, answers_valid_raw) if len(pair[1])>3]\n",
    "# questions_valid_filt, answers_valid_filt = map(list, zip(*temp))\n",
    "# print(f\"{len(questions_train_filt)} training questions and {len(questions_valid_filt)} valid questions remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train_filt = questions_train_raw[:5000]\n",
    "questions_valid_filt = questions_valid_raw[4501:5000]\n",
    "answers_train_filt = answers_train_raw[:5000]\n",
    "answers_valid_filt = answers_valid_raw[4501:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vQ = Vocab(\"Questions\")\n",
    "for sequence in [[\"<SOS>\", \"<EOS>\"]] + questions_train_filt + questions_valid_filt:\n",
    "    for token in sequence:\n",
    "        vQ.indexWord(token)\n",
    "vA = Vocab(\"Answers\")\n",
    "for sequence in [[\"<SOS>\", \"<EOS>\"]] + answers_train_filt + answers_valid_filt:\n",
    "    for token in sequence:\n",
    "        vA.indexWord(token)\n",
    "print(f\"The source vocabulary contains {len(vQ.word2index)} and the target vocabulary contains {len(vA.word2index)} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train = tokenize_questions(questions_train_filt, vQ)\n",
    "answers_train = tokenize_answers(answers_train_filt, vA)\n",
    "questions_valid = tokenize_questions(questions_valid_filt, vQ)\n",
    "answers_valid = tokenize_answers(answers_valid_filt, vA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(vQ.word2index)\n",
    "hidden_size = 124\n",
    "output_size = len(vA.word2index) \n",
    "\n",
    "dropout_E=0.0\n",
    "dropout_D=0.0\n",
    "teacher_forcing_ratio=0.0\n",
    "\n",
    "\n",
    "model = Seq2Seq(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = pretrained_w2v(init=False)\n",
    "# model = pretrain(model, vQ, vA, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => most_similar is not working after adding vector in gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 124\n",
    "print_each = 5\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "version = str(hidden_size)\n",
    "train(epochs, batch_size, print_each, lr, model, version, questions_train, answers_train, vQ, vA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "version = 124\n",
    "model.load_state_dict(torch.load(f\"model_{version}.pt\", map_location=torch.device('cpu')))\n",
    "print(f\"Loading from checkpoint: 'model_{version}.pt'\")\n",
    "\n",
    "\n",
    "max_count = 0\n",
    "for answer in answers_train:\n",
    "    if len(answer) > max_count:\n",
    "        max_count = len(answer)\n",
    "\n",
    "model.eval()\n",
    "string2stop = 'quit'\n",
    "print(f\"Type {string2stop} to finish the chat.\\n\")\n",
    "\n",
    "    \n",
    "while (True):\n",
    "    question = input(\"> \")\n",
    "    if question.strip() == string2stop:\n",
    "        break\n",
    "    \n",
    "    apply_chatbot(model, tokenize_questions([prepare_text(question)],vQ)[0].view(-1,1), vQ, vA, max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
